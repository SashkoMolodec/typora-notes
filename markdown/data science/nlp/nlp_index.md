## :scream: Natural Language Processing

[:arrow_backward:](../ds_index)

![babka](../../../src/img/babka.gif)

#### Cool links :arrow_down_small:

:one: Visualizing machine learning [blog ](http://jalammar.github.io/)(Jay Alammar):

- [The Illustrated Word2vec](http://jalammar.github.io/illustrated-word2vec/)
- [Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)](http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)

:two: James Briggs notebooks & blog:

- Notebooks from practical NLP course [here](https://github.com/jamescalam/transformers/tree/main/course) (jamescalam)
- Fine-tuning BERT's [article](https://towardsdatascience.com/how-to-train-bert-aaad00533168) (fine-tuning pretrained BERT's core, not adding new layers)

:three: GEC model [ratings](http://nlpprogress.com/english/grammatical_error_correction.html) with links (continuously updated) 
:four: Ukrainian NLP [here](https://github.com/asivokon/awesome-ukrainian-nlp)
:five: Medium blogs:

- [Tips and Tricks for your BERT based applications](https://towardsdatascience.com/tips-and-tricks-for-your-bert-based-applications-359c6b697f8e) (using BERT model with Pooler output and last hidden output)



#### Stanford NLP cs224n :arrow_down_small:

​	:ok_man: [Word vectors](stanford nlp cs224n/1_word_vectors) 
​		*(words as discrete symbols, word vectors, word2vec)*



#### French guy NLP courses :arrow_down_small:

​	:pisces: [Word embedding](nlp_word_embedding)
​		*(flow, skipgram)*

​	:cancer: [Convolutional Neural Network](nlp_cnn)
​		*(CNN for images, text, implementation)*

​	:robot: [Transformers](nlp_transformers)
​	*(scaled-dot product, attention, positional encoding, feed forward layers, add&norm, dropout, last linear, implementation)*

​	:japanese_goblin: [BERT](nlp_bert)
​	*(transfer learning - previous and existing, BERT's architecture, pre-training, MLM, NSP)*

