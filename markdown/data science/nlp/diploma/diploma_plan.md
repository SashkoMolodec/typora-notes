## План дипломки

#### Вступ

Має бути 3-6 сторінок.

Що таке нлп, які проблеми вирішує. Потім проблема власне GEC - виправлення граматики. На сьогоднішній день це перша наукова робота по розробці ГЕК для укр мови. Використовується NMT підхід, seq2seq t5 модель, синтетичні дані і файнтюннінг на анотованій версії. 

#### Основна частина

Складається як правило з трьох частин

##### Аналітичний огляд (30 сторінок)

Огляд літературних джерел, опис об'єкту та предмету дослідження, вибір напрямків дослідження

##### Загальна методика, основні методи досліджень (20 сторінок)

Теоретичні та практичні методи вирішення і їх порівняльна оцінка

##### Практична частина (20 сторінок)

Наводиться практична реалізація підходів, алгоритмів. Демонструється у виді вирішення поставленої задачі.

##### Висновки

---



##### Основні напрями

Масік: створення синтетичних даних і її дотренування на Т5 думаю

Я: можливо шось по Т5, пріпроцесинг укр анотованого, файнтюнінг моделі, евалюейт і результати




#### Розділи в темі аналітичного огляду





#### Розділи в темі загальної методики, основні методи досліджень

- Розгляд підходів до GEC в low-resources сценаріях

2. Related Work, [Grammatical Error Correction in Low-Resource Scenarios](https://arxiv.org/pdf/1910.00353v3.pdf), там буде 3 різних підходи
   десь на 2-3 сторінки розпишеться

- Підходи в англійському GEC (корпуси, на яких тренуються), таблички взагалі по даних, на яких тренується, 
- Статистика корпусів (еррор рейт, скільки там речень), токенізація (maxMatch scorer, Errant scorer)



Які трансформери використовувати: Тензор2Тензор, Лінгво чи Т5? Їх порівняння [тут](https://openreview.net/pdf?id=bzpkxS_JVsI).

> Tensor2Tensor 2019 юзається в пейпері по GEC in low-resource scenarios.

По генеруванню синтетичних даних:
[Neural Grammatical Error Correction Systems with Unsupervised Pre-training on Synthetic Data (aclanthology.org)](https://aclanthology.org/W19-4427.pdf)







